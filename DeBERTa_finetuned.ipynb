{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318083c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Obtaining dependency information for sentence_transformers from https://files.pythonhosted.org/packages/ba/20/7ef81df2e07322d95332d07c1c38c597f543c1f666d689a3153ba6fa09e3/sentence_transformers-2.6.1-py3-none-any.whl.metadata\n",
      "  Using cached sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/95/fc/661a7f06e8b7d48fcbd3f55423b7ff1ac3ce59526f146fda87a1e1788ee4/datasets-2.18.0-py3-none-any.whl.metadata\n",
      "  Using cached datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sentencepiece\n",
      "  Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/fb/12/2f5c8d4764b00033cf1c935b702d3bb878d10be9f0b87f0253495832d85f/sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting transformers<5.0.0,>=4.32.0 (from sentence_transformers)\n",
      "  Obtaining dependency information for transformers<5.0.0,>=4.32.0 from https://files.pythonhosted.org/packages/15/fc/7b6dd7e1adc0a6407b845ed4be1999e98b6917d0694e57316d140cc85484/transformers-4.39.3-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (2.0.0+cu117)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (1.11.2)\n",
      "Collecting huggingface-hub>=0.15.1 (from sentence_transformers)\n",
      "  Obtaining dependency information for huggingface-hub>=0.15.1 from https://files.pythonhosted.org/packages/05/c0/779afbad8e75565c09ffa24a88b5dd7e293c92b74eb09df6435fc58ac986/huggingface_hub-0.22.2-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (10.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.12.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (13.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/eb/3a/25c4aecb61a49d4415fd71d4f66a8a5b558dd44a52d7054ea9aa59ccbac1/xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/50/15/b56e50e8debaf439f44befec5b2af11db85f6e0f344c3113ae0be0593a91/multiprocess-0.70.16-py311-none-any.whl.metadata\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (2023.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.11.0->sentence_transformers) (3.27.6)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.11.0->sentence_transformers) (17.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.10.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers<5.0.0,>=4.32.0->sentence_transformers)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/15/0b/c09b2c0dc688c82adadaa0d5080983de3ce920f4a5cbadb7eaa5302ad251/tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.32.0->sentence_transformers)\n",
      "  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/d5/85/1e7d2804cbf82204cde462d16f1cb0ff5814b03f559fb46ceaa6b7020db4/safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Obtaining dependency information for dill<0.3.9,>=0.3.0 from https://files.pythonhosted.org/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl.metadata\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Using cached sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
      "Using cached datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "Using cached sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: sentencepiece, xxhash, safetensors, pyarrow-hotfix, dill, multiprocess, huggingface-hub, tokenizers, transformers, datasets, sentence_transformers\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "Successfully installed datasets-2.18.0 dill-0.3.8 huggingface-hub-0.22.2 multiprocess-0.70.16 pyarrow-hotfix-0.6 safetensors-0.4.3 sentence_transformers-2.6.1 sentencepiece-0.2.0 tokenizers-0.15.2 transformers-4.39.3 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers datasets sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f936757-5270-4080-af38-e96a3e5cda8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
    "from sentence_transformers import InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1662ec7d-c077-4502-9d0d-27f83ab9951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6922d1-ddae-4628-bd3d-1c398085ea38",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "872d4292-f9dc-45c1-932e-257bae6f22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the SNLI dataset\n",
    "dataset = load_dataset(\"paws\", \"labeled_final\")\n",
    "dataset = dataset.filter(lambda x: x['label'] != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee29d380-b02f-41f4-9f1e-f12a69b71e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_test = pd.DataFrame(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2c4c90b-9583-4b63-b76b-9c330de0fde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'sentence1': 'In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .',\n",
       " 'sentence2': 'In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a53b91ba-2d10-4dbe-beee-08f525687b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                                           1\n",
      "sentence1    In Paris , in October 1560 , he secretly met t...\n",
      "sentence2    In October 1560 , he secretly met with the Eng...\n",
      "label                                                        0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print first row from train set\n",
    "print(df_train.iloc[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0858d8-f652-45f7-87c6-7837c75d5e53",
   "metadata": {},
   "source": [
    "## Convert dataset to list of InputExamples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b28f3ea-ad46-4fbc-b37c-24162bdeba90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in training data: [0 1]\n",
      "Unique labels in testing data: [0 1]\n"
     ]
    }
   ],
   "source": [
    "unique_train_labels = df_train['label'].unique()\n",
    "unique_test_labels = df_test['label'].unique()\n",
    "\n",
    "print(\"Unique labels in training data:\", unique_train_labels)\n",
    "print(\"Unique labels in testing data:\", unique_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4758a7-1eac-477d-a3d1-237c21ec3805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b907d6d-a2c6-4818-bac4-c84bed858010",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = [InputExample(texts=[row['sentence1'], row['sentence2']], label=int(row['label'])) for index, row in df_train.iterrows()]\n",
    "test_examples = [InputExample(texts=[row['sentence1'], row['sentence2']], label=int(row['label'])) for index, row in df_test.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecc71a94-9bc6-4222-b198-cac27f277b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_evaluator = CEBinaryClassificationEvaluator.from_input_examples(test_examples, name='test_eval')\n",
    "except AssertionError as e:\n",
    "    print(\"Error with labels:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78c862d2-5f86-48dc-b112-73da4d50c38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<InputExample> label: 0, texts: In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .; In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .\n"
     ]
    }
   ],
   "source": [
    "print(train_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d6e1cc0-aa5b-4316-8351-df38ac97ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator = CEBinaryClassificationEvaluator.from_input_examples(test_examples, name='test_eval')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4724b757-c905-4b8e-862c-75717d09653a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49401"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6803bee0-4090-493e-8fad-8f123cd7b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 16 04:43:51 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               Off |   00000000:87:00.0 Off |                  Off |\n",
      "| 30%   31C    P8             28W /  300W |       0MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1426b22-f7d9-4dce-ac5a-96a5344256f9",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a27c7e80-23d6-47d9-900e-c17740cb77fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at cross-encoder/nli-deberta-v3-base and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model_save_path = \"./model_dump\"\n",
    "model_name = 'cross-encoder/nli-deberta-v3-base' # base model, use 'vectara/hallucination_evaluation_model' if you want to further fine-tune ours\n",
    "\n",
    "model = CrossEncoder(model_name, num_labels=1, automodel_args={'ignore_mismatched_sizes':True})\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4334e36c-9591-4260-b6e6-b34becac6ad2",
   "metadata": {},
   "source": [
    "train_batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48afdedc-6916-4249-84a7-3a9fc94b8859",
   "metadata": {},
   "source": [
    "## Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e96d1b1b-4ad9-4a6e-a8a3-d74469d59997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6808c7dc974c2b8b558f9407881c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183f1c76603c450bb40f949d5a7e1d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb9b0ba5b974ed0a404f73b4fa24eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ce4e56bfc84cf9b235be946ebdc775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3855532521ac4c379083b70b99e7a2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ede923007441d18f756726c4156b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3398adb4bc2e4f949715afbafc3344e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f538c4438a4b74b5f7b1e5920d3ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c73f6c511824627a37a0f20d3cbdf73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83eedec79e241e1a0fd6f2666000e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90c561ce4834465bf47e36f35f92aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=128)\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "model.fit(train_dataloader=train_dataloader,\n",
    "          evaluator=test_evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=10_000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path,\n",
    "          show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734053af-d55d-4b81-9d77-3249ec6b51b3",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac222112-4de9-497e-9f81-713605e263ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51441849-c05e-43cd-84a2-70cb59db9746",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"./model_output\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bb08a40-5150-4095-b8d0-181a47d883c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977efc18-b0a6-4d7a-9718-af3e22fc984c",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6677eed-5d66-4771-9361-6133dab34328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "# Load the model from the path\n",
    "model = CrossEncoder(model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba54ab6d-a261-4b1b-bfa1-778497e13e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_model(model, sentence1, sentence2):\n",
    "    # Directly use the predict method without setting eval\n",
    "    with torch.no_grad():  # It's still good practice to use no_grad() to disable gradient calculation\n",
    "        predictions = model.predict([(sentence1, sentence2)])\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "845c98e5-ee46-4a76-a37f-2d9d1827856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0.61624146]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Assuming the model is already loaded and available as `model`\n",
    "sentence1 = \"The cat sits inside the house.\"\n",
    "sentence2 = \"A cat is sitting in the kitchen.\"\n",
    "prediction = predict_with_model(model, sentence1, sentence2)\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de8caf37-792d-453e-96de-0a09d242d080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0.00872917]\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "\n",
    "# Assuming the model is already loaded and available as `model`\n",
    "sentence1 = \"The profit generated in 2022 is 26 million dollars\"\n",
    "sentence2 = \"A profit generated in 2022 is $25.00 million\"\n",
    "prediction = predict_with_model(model, sentence1, sentence2)\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15b8c3ef-3050-46af-a4ef-8d763f0e3dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the model from the saved directory\n",
    "model = CrossEncoder(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18e1c084-14c2-464d-b59e-d6eb57b60035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Score: [0.98758197]\n"
     ]
    }
   ],
   "source": [
    "def predict_with_model(model, sentence1, sentence2):\n",
    "    # Use the model's predict method which expects a list of sentence pairs\n",
    "    predictions = model.predict([(sentence1, sentence2)])\n",
    "    return predictions\n",
    "\n",
    "# Example usage\n",
    "sentence1 = \"Einstein is from Germany.\"\n",
    "sentence2 = \"Einstein is from Berlin, Gemany.\"\n",
    "prediction = predict_with_model(model, sentence1, sentence2)\n",
    "print(\"Prediction Score:\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efcc70-cde0-4e6f-a174-e60bed566a77",
   "metadata": {},
   "source": [
    "## Testing the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97788ad9-f2bd-446f-b063-645772faf53a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4171f15-8638-4fcf-ac5e-c6980fb6d6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3939cc-5fd7-4d14-a5a9-bf5fc9768756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ad367be-7015-43eb-b7fc-eb31e7cd04d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_accuracy(model, dataloader):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():  # Deactivate gradients for the following code\n",
    "        for texts, labels in dataloader:\n",
    "            predictions = model.predict(texts)  # This assumes that 'predict' returns a list or array of probabilities\n",
    "            \n",
    "            # Convert predictions to a tensor if not already one and apply threshold\n",
    "            predictions_tensor = torch.tensor(predictions)\n",
    "            predicted_labels = (predictions_tensor >= 0.5).long()  # Classify predictions based on the 0.5 threshold\n",
    "            #import pdb;pdb.set_trace()\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e22a1f4-bcb3-48d5-8354-9f1c8edb1986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    texts = [[ex.texts[0], ex.texts[1]] for ex in batch]  # Extract texts from each InputExample\n",
    "    labels = torch.tensor([ex.label for ex in batch])  # Extract labels and convert to tensor\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(test_examples, batch_size=128, collate_fn=custom_collate_fn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ad0362a-f626-44d9-873c-84173662d253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.94875\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = calculate_accuracy(model, test_dataloader)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18119e93-13b0-4212-984b-e8534eeb0183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This was a series of nested angular standards , so that measurements in azimuth and elevation could be done directly in polar coordinates relative to the ecliptic .',\n",
       " 'This was a series of nested polar scales , so that measurements in azimuth and elevation could be performed directly in angular coordinates relative to the ecliptic .']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_examples[0].texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c571049-3bc0-45f6-85a8-60a7009b4acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_examples[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d73f657-6027-439a-be21-de2c53bdff3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Obtaining dependency information for torchmetrics from https://files.pythonhosted.org/packages/f3/0e/cedcb9c8aeb2d1f655f8d05f841b14d84b0a68d9f31afae4af55c7c6d0a9/torchmetrics-1.3.2-py3-none-any.whl.metadata\n",
      "  Using cached torchmetrics-1.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.11/site-packages (from torchmetrics) (1.24.3)\n",
      "Requirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.11/site-packages (from torchmetrics) (23.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from torchmetrics) (2.0.0+cu117)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Obtaining dependency information for lightning-utilities>=0.8.0 from https://files.pythonhosted.org/packages/5e/9e/e7768a8e363fc6f0c978bb7a0aa7641f10d80be60000e788ef2f01d34a7c/lightning_utilities-0.11.2-py3-none-any.whl.metadata\n",
      "  Using cached lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (68.1.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.12.4)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.10.0->torchmetrics) (3.27.6)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.10.0->torchmetrics) (17.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "Using cached torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "Using cached lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.11.2 torchmetrics-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66c26592-b827-440a-8a89-44328d2a58cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94875\n",
      "Precision: 0.9282191780821918\n",
      "Recall: 0.9581447963800905\n",
      "F1 Score: 0.9429446145282494\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_metrics(model, dataloader):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    with torch.no_grad():  # Deactivate gradients for the following code\n",
    "        for texts, labels in dataloader:\n",
    "            predictions = model.predict(texts)  # Assumes 'predict' returns a list or array of probabilities\n",
    "            \n",
    "            predictions_tensor = torch.tensor(predictions)\n",
    "            predicted_labels = (predictions_tensor >= 0.5).long()  # Classify predictions based on the 0.5 threshold\n",
    "            \n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            # Calculate true positives, false positives, and false negatives\n",
    "            true_positives += ((predicted_labels == 1) & (labels == 1)).sum().item()\n",
    "            false_positives += ((predicted_labels == 1) & (labels == 0)).sum().item()\n",
    "            false_negatives += ((predicted_labels == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "    # Calculate accuracy, precision, recall, and F1 score\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Example usage assuming the dataloader and model are set up:\n",
    "accuracy, precision, recall, f1 = calculate_metrics(model, test_dataloader)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a24c61-1272-4e47-b28d-1101d3eeaabd",
   "metadata": {},
   "source": [
    "## Uploading it on Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46ca6603-ec61-40dc-85cd-2a9d9338c2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.11/site-packages (0.22.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (2023.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11149f28-6020-4541-b29b-489767105c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): "
     ]
    }
   ],
   "source": [
    "!huggingface-cli login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71cbadb-deb0-40d5-abe6-c310248559d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import Repository, HfFolder\n",
    "\n",
    "# Define paths and repository details\n",
    "local_dir = 'training'  # Local directory where your model and README.md are saved\n",
    "repo_name = 'hallucination_detector'  # Repository name on Hugging Face Hub\n",
    "username = 'Varun-Sayapaneni'  # Your Hugging Face username\n",
    "\n",
    "# Create or use existing repository\n",
    "repo = Repository(local_dir=local_dir, clone_from=f'{username}/{repo_name}', use_auth_token=True)\n",
    "\n",
    "# If you need to add files, you can do it here\n",
    "# For example:\n",
    "# shutil.copy('my_model.bin', local_dir)\n",
    "\n",
    "# Commit and push files to the repository\n",
    "repo.git_add('*')\n",
    "repo.git_commit(\"Initial model upload\")\n",
    "repo.git_push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc608716-14ac-4c7e-a84d-8b0945bbb2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
